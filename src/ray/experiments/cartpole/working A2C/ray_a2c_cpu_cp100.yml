ray_a2c_cpu_cp100:
  run: A2C
  env: Cartpole2-v11
  local_dir: "src/results/ray/cartpole/A2C"  
  checkpoint_freq: 10000
  checkpoint_at_end: True
  # number of iterations
  stop:
    training_iteration: 1000
    # all other hyperparameters (e.g. length of one iteration)
  config:
    evaluation_num_episodes: 0
#   adam_epsilon: 0.00015
#   buffer_size: 10000
    # we want to evaluate manually, but still need the evaluator
    evaluation_interval: 1
    lr: 0.0000625
    num_workers: 4
    #num_envs: 5
    #num_envs_per_worker: 1
    remote_worker_envs: False
    train_batch_size: 64
    sample_batch_size: 4
    gamma: 0.99           #discount factor
    # grad_clip: 40.0       #gradient clipping
    entropy_coeff: 0.01   # entropy penalty parameter Î² #entropy loss function
    vf_loss_coeff: 0.25   #
    sample_async: True
    # Beta parameter for sampling from prioritized replay buffer.
    #prioritized_replay_beta: 0.4
    # Fraction of entire training period over which the beta parameter is
    # annealed
    #beta_annealing_fraction: 0.2
#   epsilon: 0.1
#   alpha: 0.99
#   prioritized_replay: True
#   timesteps_per_iteration: 1000 #00 #    # evaluation_num_episodes: 1000
#   hiddens: [100,100]
#   num_atoms: 100
#   dueling: True
#   double_q: True
#   n_step: 1
#   target_network_update_freq: 1000
#    exploration_final_eps: 0.01
#    exploration_fraction: 0.9
#    learning_starts: 0
#   schedule_max_timesteps: 1000000
#   v_min: -50.0
#  v_max: 50.0
